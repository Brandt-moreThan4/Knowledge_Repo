demo()
library(readr)
dataset <- read_csv(NULL)
View(dataset)
5+5
8+8
16*9
[1,2]
"lolol"
"lolol" + "haha"
help.start()
lol = 5
lol
lol <- 7
lol + lol
print(x*x)
for (x in 1:10) {
print(x)
}
for (x in 1:100) {
print(x)
}
print(x * x)
print()
print(x)
for (x in 1:10) {
print(x)
}
for (x in 1:100) {
print(x)
}
print(x + x)
class("lol")
print(y)
y <- x + 4
print("lol")
print(x)
print(y)
y <- x * x
print(y)
for (x in 1:10) {
y <- x * x
print(y)
}
y = 2
for (x in 1:10) {
y <- x * x
print(y)
}
y = 2
for (x in 1:10) {
y <- x * x
print(y)
}
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
for (x in 1:10) {
y <- x * x
print(y)
}
x= 5
y = 2
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
5
lol = 5
lol
lol = 7
lol
library(readxl)
Galton_father_son_height_data <- read_excel("C:/Users/User/OneDrive/Desktop/stats_boot/Galton father-son height data.xlsx")
View(Galton_father_son_height_data)
help.start()
x = 5
x
exit()
exit
source('~/.active-rstudio-document', echo=TRUE)
x
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
library(readxl)
iris_data <- read_excel("C:/Users/User/OneDrive/Desktop/data_explore/data/iris_data.xlsx")
View(iris_data)
iris_data
summarize(iris_data)
summary(iris_data)
iris_data$sepal_length
mean(iris_data$sepal_length)
mean(1,4)
mean(1,4,5)
?mean
mean(c(2,3,4,5))
iris
iris$Sepal.Length
iris_data = iris
view(iris)
View(iris_data)
View(iris_data)
view(iris)
install.packages("tinytex")
tinytex::install_tinytex()
getwd()
setwd("C:/Users/User/OneDrive/Desktop/Unsupervised/Notes")
source("C:/Users/User/OneDrive/Desktop/Unsupervised/Notes/PCA/R for basic PCA.R", echo=TRUE)
prcomp?
5
lm?
5
sum(5,5)
sum?
5
?sum
?prcomp
library(readxl)
AustinApartmentRent <- read_excel("PCA/AustinApartmentRent.xls")
View(AustinApartmentRent)
View(AustinApartmentRent)
fit <- prcomp(AustinApartmentRent[2:9], scale=TRUE)
fit
View(AustinApartmentRent)
AustinApartmentRent[2:9]
fit <- prcomp(AustinApartmentRent, scale=TRUE)
fit
names(fit) # names of vars in object fit
summary(fit) # print proportion of variance accounted for
fit$sdev^2 # variance explained
plot(fit,type="lines") # scree plot
fit$rotation # eigenvectors of corr matrix
biplot(fit, scale = 0) # biplot PC1 v PC2, scale=0 makes arrows scaled to rep loadings
source("C:/Users/User/OneDrive/Desktop/Unsupervised/Notes/PCA/R for basic PCA.R", echo=TRUE)
setwd("C:/Users/User/OneDrive/Desktop/Code/Knowledge_Repo/Tutorials/R_Reference")
knitr::opts_chunk$set(echo = TRUE)
#Calculate averages
rapidcity %>%
summarize(avg_temp = mean(Temp))
knitr::opts_chunk$set(echo = TRUE)
# Import libraries
library(tidyverse)
library(ggplot2)
# Load one of R's built-in data sets about cars
data(mtcars)
# Fit a straight line for mpg vs hp and plot the result.
mpg_model = lm(mpg ~ hp, data=mtcars) # Fit Linera Model
plot(mtcars$hp, mtcars$mpg) # Plot Linear Model
abline(mpg_model) # Draw the best fit line
print(coef(mpg_model)) # Print the coefficients to command line
# library(readr)
# tv_df <- read_csv("data/tvshows.csv")
tv_df = read.csv('data/tvshows.csv', header=TRUE)
# View(tv_data)
str(tv_df) # Basic dataframe info
head(tv_df) # View the first 6 rows
xtabs(~Genre + Duration, data=tv_df) # Cross tabulate the data
# Get the Average audience size  by genre
tv_df %>% group_by(Genre) %>% summarise(mean_GRP = mean(GRP))
# Plot engagement by genre
ggplot(tv_df) + geom_point(aes(x=GRP,y=PE,color=Genre))
acl_df = read.csv('data/aclfest.csv', header=TRUE)
str(acl_df)
head(acl_df)
# How many bands played at lallapalooza
palooza_counts = xtabs(~lollapalooza, data=acl_df)
print(palooza_counts)
palooza_proportions = prop.table(palooza_counts)
palooza_proportions
# Same as above but with piping
xtabs(~lollapalooza,data=acl_df) %>% prop.table %>% round(3)
# Joint probs of acl and lolla
xtabs(~acl + lollapalooza, data=acl_df)
xtabs(~acl + lollapalooza, data=acl_df) %>% prop.table %>%  round(3)
# Or probability of playing at Bonnaroo or Coachella
# Add margins, just adds the total rows
xtabs(~bonnaroo + coachella, data=acl_df) %>%
prop.table %>%
addmargins
tvshows = read.csv('data/tvshows.csv', header=TRUE)
# head(tvshows)
power_christmas2015 = read.csv('data/power_christmas2015.csv', header=TRUE)
# head(power_christmas2015)
rapidcity = read.csv('data/rapidcity.csv', header=TRUE)
# head(rapidcity)
kroger = read.csv('data/kroger.csv', header=TRUE)
# head(kroger)
car_class_summaries = read.csv('data/car_class_summaries.csv', header=TRUE)
# head(car_class_summaries)
# Scatter Plot with shape as a facet
ggplot(tvshows) +
geom_point(aes(x=GRP, y=PE, shape=Genre))
# Scatter Plot with size as a facet.
# This makes more sense to do on a quantitative variable though.
ggplot(tvshows) +
geom_point(aes(x=GRP, y=PE, size=Genre))
# Facet plot by different genre
ggplot(tvshows) +
geom_point(aes(x=GRP, y=PE)) +
facet_wrap(~Genre)
# Line plot
ggplot(power_christmas2015) +
geom_line(aes(x=hour, y=ERCOT))
# Historgram
ggplot(rapidcity) +
geom_histogram(aes(x=Temp),bins=30)
# Histogram faceted by month
ggplot(rapidcity) +
geom_histogram(aes(x=Temp), binwidth=3) +
facet_wrap(~Month)
# Histogram with probabilities
ggplot(rapidcity) +
geom_histogram(aes(x=Temp, y=..density..), binwidth=3)
# Boxplots, grouped by city
ggplot(kroger) +
geom_boxplot(aes(x = city, y = vol))
# Demand graph
ggplot(kroger) +
geom_point(aes(x = vol, y = price))
# Bar plot of summary stats
# Use geom_col when the data is already summarized
ggplot(car_class_summaries) +
geom_col(aes(x=class, y=average_cty))
# Use geom_bar when you need R to do the counting of the raw data
data(mpg) # read in the dataset
ggplot(mpg) +
geom_bar(aes(x=class))
# Adding better labels to plot and adjusting the font size
ggplot(kroger) +
geom_boxplot(aes(x = city, y = vol)) +
labs(x="Location of Kroger store",
y="Weekly sales volume (packages sold)",
title="Weekly cheese sales at 11 U.S. Kroger stores") +
theme(axis.text = element_text(size = 8))
#Calculate averages
rapidcity %>%
summarize(avg_temp = mean(Temp))
#Calculate averages
rapidcity %>%
summarize(avg_temp = mean(Temp))
#Calculate averages
rapidcity %>%
summarize(avg_temp = mean(Temp))
#Calculate averages
rapidcity %>%
summarize(avg_temp = mean(Temp),
median_temp = median(Temp))
#Calculate summary stats
rapidcity %>%
summarize(avg_temp = mean(Temp),
median_temp = median(Temp),
sd_temp = sd(Temp))
#Calculate summary stats
rapidcity %>%
summarize(avg_temp = mean(Temp),
median_temp = median(Temp),
sd_temp = sd(Temp),
iqr_temp = IQR(Temp))
#Calculate summary stats
rapidcity %>%
summarize(avg_temp = mean(Temp),
median_temp = median(Temp),
sd_temp = sd(Temp),
iqr_temp = IQR(Temp),
min_temp = min(Temp),
max_temp = max(Temp),
q05_temp = quantile(Temp, 0.05),
q95_temp = quantile(Temp, 0.95)) %>%
round(1)
# standarize the data
rapidcity = rapidcity %>%
mutate(z = (Temp - mean(Temp))/sd(Temp))
rapidcity
# Calculate summary statistic by group
rapidcity %>%
group_by(Month) %>%
summarize(avg_temp = mean(Temp),
sd_temp = sd(Temp),
q05_temp = quantile(Temp, 0.05),
q95_temp = quantile(Temp, 0.95)) %>%
round(1)
?summarize
rapidcity_summary = rapidcity %>%
group_by(Month) %>%
summarize(avg_temp = mean(Temp),
prop_freeze = sum(Temp <= 32)/n())
rapidcity_summary = rapidcity %>%
group_by(Month) %>%
summarize(avg_temp = mean(Temp),
prop_freeze = sum(Temp <= 32)/n())
rapidcity_summary
ggplot(rapidcity_summary) +
geom_col(aes(x=factor(Month), y=avg_temp))
rapidcity_summary
ggplot(rapidcity_summary) +
geom_col(aes(x=factor(Month), y=avg_temp))
ggplot(rapidcity_summary) +
geom_col(aes(x=factor(Month), y=prop_freeze))
# Filter data by year before computing summary statistics
rapidcity %>%
filter(Year >= 2006 & Year <= 2009) %>%
group_by(Month) %>%
summarize(avg_temp = mean(Temp),
sd_temp = sd(Temp)) %>%
round(1)
rapidcity %>%
filter(Year == 2009) %>%
select(Month, Day, Temp) %>%
head
rapidcity %>%
filter(Year == 2009) %>%
select(Month, Day, Temp)
#Alternatively, using a  negative sign will remove certain columns.
# Below achieves same result as above
rapidcity %>%
filter(Year == 2009) %>%
select(-Year)
rapidcity %>%
filter(Year == 2009) %>%
select(Month, Day, Temp)
#Alternatively, using a  negative sign will remove certain columns.
# Below achieves same result as above
rapidcity %>%
filter(Year == 2009) %>%
select(-Year,-z)
#Alternatively, using a  negative sign will remove certain columns.
# Below achieves same result as above
rapidcity %>%
filter(Year == 2009) %>%
select(-Year,-z)
# Mutate allows you to add new variables from old ones
rapidcity_augmented = rapidcity %>%
mutate(Summer = ifelse(Month == 6 | Month == 7 | Month == 8,
yes="summer", no="not_summer"))
rapidcity_augmented
# Use arrange for sorting
rapidcity %>%
arrange(Temp) %>%
head(10)
# Use arrange for sorting
rapidcity %>%
arrange(Temp)
arrange(desc(Temp)
# The default is ascending. We can change that
rapidcity %>%
# The default is ascending. We can change that
rapidcity %>%
arrange(desc(Temp))
# Find the 5 coldest months
rapidcity %>%
group_by(Year, Month) %>%
summarize(avg_temp = mean(Temp),
coldest_day = min(Temp),
warmest_day = max(Temp)) %>%
arrange(avg_temp) %>%
head(5) %>%
round(1)
titanic
data("Titanic")
titanic
Titanic
titanic = Titanic
titanic
head(titanic)
#install.packages('dplyr')
library(dplyr) # Loading Dplyr package
titanic = read.csv("data/titanic.csv") # Load in the titanic data set
head(titanic)
# A bit more complex wranlging
surv_adults = titanic %>%
mutate(age_bracket = ifelse(age >= 18,
yes="adult", no="child")) %>%
filter(age_bracket == "adult") %>%
group_by(sex, passengerClass) %>%
summarize(total_count = n(),
surv_count = sum(survived == 'yes'),
surv_pct = surv_count/total_count)
surv_adults
ggplot(surv_adults) +
geom_col(aes(x=sex, y=surv_pct)) +
facet_wrap(~passengerClass, nrow=1)
ggplot(surv_adults) +
geom_col(aes(x=sex, y=surv_pct,fill=sex)) +
facet_wrap(~passengerClass, nrow=1)
toyimports  = read.csv("data/toyimports.csv") # Load in the titanic data set
head(toyimports )
toyimports
country_totals = toyimports %>%
group_by(partner_name) %>%
summarize(total_dollar_value = sum(US_report_import)) %>%
arrange(desc(total_dollar_value))
country_totals
top3_partner_names = c('China', 'Denmark', 'Canada')
top3_partner_names[0]
top3_partner_names(0)
top3_partner_names[1]
top3_partner_names[1:2]
top3_partner_names[1:4]
top3_partner_names[1:8]
top3_byyear = toyimports %>%
filter(partner_name %in% top3_partner_names) %>%
group_by(year, partner_name) %>%
summarize(yearly_dollar_value = sum(US_report_import))
head(top3_byyear)
ggplot(top3_byyear) +
geom_line(aes(x=year, y=yearly_dollar_value, color=partner_name)) +
scale_color_brewer(type='qual') +
scale_y_log10() +
scale_x_continuous(breaks = 1996:2005) +
labs(x="Year", y = "Dollar value of imports (log scale)",
title="Toy imports from the U.S.'s top-3 partners, 1996-2005")
library(mosaic)
mean(~age,data=titanic)
mean(age ~ sex, data=titanic)
prop(~survived, data=titanic)
favstats(age ~ sex, data=titanic) # combo of all the good stuff
